{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"assign.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.pointbiserialr(df[\"y\"], df[\"x9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='y',palette='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"x7\", y=\"x9\", hue=\"y\",data=df, palette=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='x1',y='x9',data=df,kind='scatter', hue='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='x10',y='x17',data=df,kind='scatter', hue='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.violinplot(x='x2',y='x13',data=df, hue='y', palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Process null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x4'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x5'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x6'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x7'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['x8'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can conclude that null value will include NaN and unknown for categorical data. Besides, there will have NaN value for x13 for numerical data. So we need to tell pandas to treat these values as null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_values = ['NaN', \"unknown\"]\n",
    "na_values = ['NaN', \"unknown\"]\n",
    "data = pd.read_csv(\"assign.csv\", na_values=na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().plot(kind='bar', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = data['x14'].median()\n",
    "data['x14'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequently value for x3\n",
    "data.mode().loc[:,['x2', 'x3', 'x4', 'x5', 'x6', 'x8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill x3, x2, x8 with most frequently occuring value\n",
    "data['x2'].fillna(\"admin\", inplace=True)\n",
    "data['x3'].fillna(\"married\", inplace=True)\n",
    "data['x4'].fillna(\"university\", inplace=True)\n",
    "data['x5'].fillna(\"yes\", inplace=True)\n",
    "data['x6'].fillna(\"no\", inplace=True)\n",
    "data['x8'].fillna(\"cell\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"x11\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are only 3 percent of data that is not 999.\n",
    "# In other word, there are 97 percent of data value is 999\n",
    "# Therefore, this column doesn't have any value for modeling.\n",
    "data['x11'][data['x11'] != 999].count() / data['x11'][data['x11'] == 999].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"x11\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"x12\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 15 percent of data value is not 0\n",
    "# In other word, there are 85 percent of data value is 0\n",
    "# This column may have some value for modeling, however \n",
    "# we would drop it for fine tuning section to see whether the performance of model improve if we drop this column.\n",
    "data['x12'][data['x12'] != 0].count() / data['x12'][data['x12'] == 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical data to numerical data using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train data to categorical and numerical\n",
    "X_train_num = X_train.drop(['x2', 'x3', 'x4', 'x5', 'x6', 'x7' ,'x8'], axis=1)\n",
    "X_train_cat = X_train[['x2', 'x3', 'x4', 'x5', 'x6', 'x7' ,'x8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing instances\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "scalar = StandardScaler()\n",
    "encode = OneHotEncoder()\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"scaler\", scalar, X_train_num.columns),\n",
    "        (\"encode\", encode, X_train_cat.columns),\n",
    "    ])\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "X_test_preapred = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After deal with categorical and numerical data, we need to resample the data since \n",
    "# it has unbalance data\n",
    "# So, false value is far greater than true value\n",
    "y.value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "This resampling technique is temparory, more resampling technique will be examine in fine tune section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to resample the data to balance the data\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=1)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_prepared, y_train)\n",
    "\n",
    "ax = y_train_resampled.value_counts().plot.pie()\n",
    "ax.set_title(\"undersampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=500, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sgd_clf.predict(X_test_preapred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 fine tuing and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem like this model is not overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing instances\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "scalar = StandardScaler()\n",
    "encode = OneHotEncoder()\n",
    "full_pipeline = ColumnTransformer([\n",
    "        # polynomial transformation\n",
    "        (\"poly\", poly, X_train_num[['x10', 'x13']].columns),\n",
    "        (\"scaler\", scalar, X_train_num.columns),\n",
    "        (\"encode\", encode, X_train_cat.columns),\n",
    "    ])\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "X_test_preapred = full_pipeline.transform(X_test)\n",
    "\n",
    "# Now we need to resample the data to balance the data\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=1)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_prepared, y_train)\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=500, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train_resampled, y_train_resampled)\n",
    "predictions = sgd_clf.predict(X_train_resampled)        \n",
    "print(classification_report(y_train_resampled, predictions))\n",
    "print(confusion_matrix(y_train_resampled, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try use different resampling technique\n",
    "# But it seem like no different\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "# Now we need to resample the data to balance the data\n",
    "enn  = EditedNearestNeighbours()\n",
    "X_train_resampled, y_train_resampled = enn.fit_resample(X_train_prepared, y_train)\n",
    "\n",
    "ax = y_train_resampled.value_counts().plot.pie()\n",
    "ax.set_title(\"undersampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing instances\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "scalar = StandardScaler()\n",
    "encode = OneHotEncoder()\n",
    "X_train_num = X_train_num.drop(\"x12\", axis=1) # Fine tune \n",
    "full_pipeline = ColumnTransformer([\n",
    "        # polynomial transformation\n",
    "        (\"poly\", poly, X_train_num[['x10', 'x13']].columns), # Fine tune \n",
    "        (\"scaler\", scalar, X_train_num.columns),\n",
    "        (\"encode\", encode, X_train_cat.columns),\n",
    "    ])\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "X_test_preapred = full_pipeline.transform(X_test)\n",
    "\n",
    "# Now we need to resample the data to balance the data\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=1)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_prepared, y_train)\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=500, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train_resampled, y_train_resampled)\n",
    "predictions = sgd_clf.predict(X_test_preapred)        \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
